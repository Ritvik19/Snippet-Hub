{"Title":{"0":"ANN","1":"CNN (text)","2":"CSS Variables","3":"CustomScrollBar","4":"Deal with Large Numbers","5":"Directory Manipulations","6":"Directory Walk","7":"Docstring","8":"Dynamic Attributes","9":"Eval()","10":"Flatten a list","11":"Function Annotations","12":"Generate Secret Key","13":"IgnoreWarnings","14":"Image Shadow","15":"List Filter","16":"Modules from Directory","17":"Most Common Words","18":"Multiple files single 'with'","19":"Named Tuple","20":"Pandas Display Settings","21":"Password in command line","22":"RNN (text)","23":"Shebang","24":"Stopwords","25":"TextPreprocessing","26":"TimeStamp","27":"Tracking Loops","28":"Trim in JavaScript","29":"Unpacking","30":"VADER Sentiment Analysis","31":"Webscraping","32":"Word Cloud","33":"is vs =="},"Description":{"0":"(python)","1":"(python)","2":"(web)","3":"(web)","4":"(python)","5":"(python)","6":"(python)","7":"(python)","8":"(python)","9":"(python)","10":"(python)","11":"(python)","12":"(python)","13":"(python)","14":"(web)","15":"(web)","16":"(python)","17":"In a DataFrame (python)","18":"(python)","19":"(python)","20":"(python)","21":"(python)","22":"(python)","23":"Shebang line for anaconda virtual environments (python)","24":"List of Stopwords(text)","25":"(python)","26":"(python)","27":"(python)","28":"(web)","29":"(python)","30":"(python)","31":"Using bs4 (python)","32":"(python)","33":"(python)"},"Code":{"0":"from keras.models import Sequential\nfrom keras.laeyrs import Dense, Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers\n# optimizers.SGD()\n# optimizers.RMSProp()\n# optimizers.Adagrad()\n# optimizers.Adadelta()\n# optimizers.Adam()\n\nes = EarlyStopping(monitor='loss', mode='min', verbose=1)\nfilepath = \"model.h5\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n\ndef build_network():\n    model = Sequential()\n    model.add(Dense(160,input_dim=40, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(320, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(365, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(125, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(25, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=, metrics=['accuracy'])\n    model.summary()\n    return model\n\nmodel = build_network()\n\nmodel.fit(scaled_data, train_label[0], validation_split=0.3, epochs=25, callbacks=[es, ckpt])\n\n\nfrom keras.models import load_model\nnew_model = load_model(\"model.h5\")","1":"from keras.models import Sequential\nfrom keras.laeyrs import Dense, Dropout, Flatten, Embedding, Convolution1D, MaxPooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import optimizers\n# optimizers.SGD()\n# optimizers.RMSProp()\n# optimizers.Adagrad()\n# optimizers.Adadelta()\n# optimizers.Adam()\n\nvocabulary_size = 7500\npad_length = 1000\ntokenizer = Tokenizer(num_words= vocabulary_size)\ntokenizer.fit_on_texts(X)\nsequences = tokenizer.texts_to_sequences(X)\npadded_sequences = pad_sequences(sequences, maxlen=pad_length)\n\nes = EarlyStopping(monitor='loss', mode='min', verbose=1)\nfilepath = \"model.h5\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n\ndef build_network():\n    model = Sequential()\n    model.add(Embedding(vocabulary_size, 1024, input_length=pad_length))\n    model.add(Convolution1D(1024, kernel_size=5, activation='tanh', strides=2))\n    model.add(MaxPooling1D(pool_size=5))\n    model.add(Dense(160,input_dim=40, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(320, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(365, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(125, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(25, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=, metrics=['accuracy'])\n    model.summary()\n    return model\n\nmodel = build_network()\n\nmodel.fit(scaled_data, train_label[0], validation_split=0.3, epochs=25, callbacks=[es, ckpt])\n\n\nfrom keras.models import load_model\nnew_model = load_model(\"model.h5\")","2":":root {\n  --main-bg-color: coral; \n}\n\n#div1 {\n  background-color: var(--main-bg-color); \n}","3":"\/* width *\/\n::-webkit-scrollbar {\n  width: 10px;\n}\n\n\/* Track *\/\n::-webkit-scrollbar-track {\n  background: #f1f1f1; \n}\n\n\/* Handle *\/\n::-webkit-scrollbar-thumb {\n  background: #888; \n}\n\n\/* Handle on hover *\/\n::-webkit-scrollbar-thumb:hover {\n  background: #555; \n}","4":"num1 = 100_000_000_000  # Visually chunk large numbers\nnum2 =   1_000_000_000  # without affecting the functionality\ntotal = num1 + num2\nprint(f'{total:,}')     # 101,000,000,000","5":"# Current working directory\nos.getcwd()\n\n# Change the CWD\nos.chdir(PATH)\n\n# Delete any directory\nos.rmdir(PATH)\n\n# Create a new directory\nos.mkdir(PATH)\n\n# Rename a directory\nos.rename(OLD, NEW)\n\n# Delete entire directory tree\nshutil.rmtree(PATH)\n\n# Copy entire directory tree\nshutil.copytree(SRC, DST)\n\n# Move file or directory\nshutil.move(SRC, DST)","6":"import os\n\nfor folderName, subfolders, filenames in os.walk('DIRECTORY'):\n    print('The current folder is ' + folderName)\n\n    for subfolder in subfolders:\n        print('SUBFOLDER OF ' + folderName + ': ' + subfolder)\n    for filename in filenames:\n        print('FILE INSIDE ' + folderName + ': '+ filename)\n\n    print('')","7":"def add(a, b):\n   \"\"\"this function takes 2 numbers and return their sum\"\"\"\n   return a+b\n\nprint(add.__doc__)\n\n# Output:\n# this function takes 2 numbers and return their sum\n\n\nclass Car():\n   \"\"\"this is a class\"\"\"\n   pass\n\nprint(Car.__doc__)\n\n# Output:\n# this is a class","8":"class Person():\n    pass\n\nperson = Person()\n\np_key = 'name'\np_value = 'Ritvik'\n\n\nsetattr(person, p_key, p_value)\nname = getattr(person, p_key)","9":"# it is a builtin function that allows us to\n# execute arbitrry strings in python\n\nadd = \"1+5+6\"\ndisplay = \"print('Hello')\"\n\nprint (eval(add))  # 12\neval(display)      # Hello","10":"flat_list = [item for sublist in l for item in sublist]","11":"def power(a: int, b:int) ->int:\n    return a**b\n\nprint(power.__annotations__)\n\n# Output:\n# {'a': <class 'int'>, 'b': <class 'int'>, 'return': <class 'int'>}","12":">>> import secrets\n>>> secrets.token_hex(16)\n'79e9d3b5d183b6e620e3776f77d95f4b'","13":"import warnings\r\nwarnings.filterwarnings(\"ignore\")","14":"filter: drop-shadow(0 15px 17px rgba(38,38,143,.37));","15":"function listFilter(inputEl, listEl, element) {\n    var input, filter, ul, li, a, i, txtValue;\n    input = document.getElementById(inputEl);\n    filter = input.value.toUpperCase();\n    ul = document.getElementById(listEl);\n    li = ul.getElementsByTagName(\"li\");\n    for (i = 0; i < li.length; i++) {\n        a = li[i].getElementsByTagName(element)[0];\n        txtValue = a.textContent || a.innerText;\n        if (txtValue.toUpperCase().indexOf(filter) > -1) {\n            li[i].style.display = \"\";\n        } else {\n            li[i].style.display = \"none\";\n        }\n    }\n}","16":"import sys, os\nsys.path.append(os.path.join(sys.path[0], 'modules'))\n\nimport module1","17":"pd.Series(' '.join(df['text']).split()).value_counts()[:n]","18":"with     open('a.txt') as a,     open('b.txt', 'w') as b:\n    for line in a:\n        b.write(line)","19":"from collections import namedtuple\n\nColor = namedtuple('Color', ['red', 'green', 'blue'])\ncolor = Color(55,155,255)\n\nprint(color[0])    # 55\nprint(color.red)   # 55","20":"pd.options.display.max_columns = 10\npd.options.display.max_colwidth = -1","21":"from getpass import getpass\nusername = input('Enter Username...')   # User Input is displayed in the commandline\npassword = getpass('Enter Password...') # User Input is not displayed in the commandline","22":"from keras.models import Sequential\nfrom keras.laeyrs import Dense, Dropout, LSTM, Embedding\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import optimizers\n# optimizers.SGD()\n# optimizers.RMSProp()\n# optimizers.Adagrad()\n# optimizers.Adadelta()\n# optimizers.Adam()\n\nvocabulary_size = 7500\npad_length = 1000\ntokenizer = Tokenizer(num_words= vocabulary_size)\ntokenizer.fit_on_texts(X)\nsequences = tokenizer.texts_to_sequences(X)\npadded_sequences = pad_sequences(sequences, maxlen=pad_length)\n\nes = EarlyStopping(monitor='loss', mode='min', verbose=1)\nfilepath = \"model.h5\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n\ndef build_network():\n    model = Sequential()\n    model.add(Embedding(vocabulary_size, 1024, input_length=pad_length))\n    model.add(LSTM(1024, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(160,input_dim=40, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(320, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(365, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(125, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(25, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=, metrics=['accuracy'])\n    model.summary()\n    return model\n\nmodel = build_network()\n\nmodel.fit(scaled_data, train_label[0], validation_split=0.3, epochs=25, callbacks=[es, ckpt])\n\n\nfrom keras.models import load_model\nnew_model = load_model(\"model.h5\")","23":"#!D:\\Users\\Ritvik\\Anaconda3\\envs\\datascience\\python.exe","24":"a\nabout\nabove\nacross\nafter\nafterwards\nagain\nagainst\nall\nalmost\nalone\nalong\nalready\nalso\nalthough\nalways\nam\namong\namongst\namoungst\namount\nan\nand\nanother\nany\nanyhow\nanyone\nanything\nanyway\nanywhere\nare\naround\nas\nat\nback\nbe\nbecame\nbecause\nbecome\nbecomes\nbecoming\nbeen\nbefore\nbeforehand\nbehind\nbeing\nbelow\nbeside\nbesides\nbetween\nbeyond\nbill\nboth\nbottom\nbut\nby\ncall\ncan\ncannot\ncant\nco\ncomputer\ncon\ncould\ncouldnt\ncry\nde\ndescribe\ndetail\ndo\ndone\ndown\ndue\nduring\neach\neg\neight\neither\neleven\nelse\nelsewhere\nempty\nenough\netc\neven\never\nevery\neveryone\neverything\neverywhere\nexcept\nfew\nfifteen\nfify\nfill\nfind\nfire\nfirst\nfive\nfor\nformer\nformerly\nforty\nfound\nfour\nfrom\nfront\nfull\nfurther\nget\ngive\ngo\nhad\nhas\nhasnt\nhave\nhe\nhence\nher\nhere\nhereafter\nhereby\nherein\nhereupon\nhers\nherse\"\nhim\nhimse\"\nhis\nhow\nhowever\nhundred\ni\nie\nif\nin\ninc\nindeed\ninterest\ninto\nis\nit\nits\nitse\"\nkeep\nlast\nlatter\nlatterly\nleast\nless\nltd\nmade\nmany\nmay\nme\nmeanwhile\nmight\nmill\nmine\nmore\nmoreover\nmost\nmostly\nmove\nmuch\nmust\nmy\nmyse\"\nname\nnamely\nneither\nnever\nnevertheless\nnext\nnine\nno\nnobody\nnone\nnoone\nnor\nnot\nnothing\nnow\nnowhere\nof\noff\noften\non\nonce\none\nonly\nonto\nor\nother\nothers\notherwise\nour\nours\nourselves\nout\nover\nown\npart\nper\nperhaps\nplease\nput\nrather\nre\nsame\nsee\nseem\nseemed\nseeming\nseems\nserious\nseveral\nshe\nshould\nshow\nside\nsince\nsincere\nsix\nsixty\nso\nsome\nsomehow\nsomeone\nsomething\nsometime\nsometimes\nsomewhere\nstill\nsuch\nsystem\ntake\nten\nthan\nthat\nthe\ntheir\nthem\nthemselves\nthen\nthence\nthere\nthereafter\nthereby\ntherefore\ntherein\nthereupon\nthese\nthey\nthick\nthin\nthird\nthis\nthose\nthough\nthree\nthrough\nthroughout\nthru\nthus\nto\ntogether\ntoo\ntop\ntoward\ntowards\ntwelve\ntwenty\ntwo\nun\nunder\nuntil\nup\nupon\nus\nvery\nvia\nwas\nwe\nwell\nwere\nwhat\nwhatever\nwhen\nwhence\nwhenever\nwhere\nwhereafter\nwhereas\nwhereby\nwherein\nwhereupon\nwherever\nwhether\nwhich\nwhile\nwhither\nwho\nwhoever\nwhole\nwhom\nwhose\nwhy\nwill\nwith\nwithin\nwithout\nwould\nyet\nyou\nyour\nyours\nyourself\nyourselves\nwhom\nm\ncouldn't\nuntil\nduring\nunder\nwasn\nboth\nout\ndidn\nbefore\naren't\nweren\nwhy\neach\nmightn\ndoesn\nbelow\nshould\nhers\nherself\nwho\ntoo\nmost\nand\nso\nabout\nshould've\nme\nyou'll\nno\nup\nshouldn\ncan\noff\nthan\njust\nonce\nain\nisn\nwon't\nthat'll\nthose\ndon\nhadn't\nisn't\nwhich\nshouldn't\nto\nthemselves\nyourself\nwas\non\nother\no\nhad\nher\nourselves\nmightn't\nhe\ndoes\nany\nyour\nthese\ndoing\nan\nby\nthrough\nwe\nfrom\nmy\nits\nit\nhaven\nma\nhasn't\nbeing\nve\nsome\nshan't\nover\nmustn't\nhimself\ntheirs\nwhere\nif\nmyself\ndo\ndon't\nwouldn't\nthe\nof\nneedn\nwere\naren\nin\nwasn't\nfew\nnow\nmustn\ns\ny\nyou're\ntheir\nbeen\nour\ni\nbecause\nnot\nbe\nthen\nagainst\nd\nhaven't\nhis\nagain\nhow\nall\nas\nweren't\nmore\nwouldn\nwill\ninto\nthis\nyou\nshe's\nare\nyourselves\ndidn't\nabove\nown\nneedn't\nwith\nyou'd\nhave\nam\nfurther\nthere\nvery\nnor\ndoesn't\nwhat\ndid\nthat\nwhile\nthem\nsuch\nhadn\nshe\nbetween\nhaving\ncouldn\nhas\na\nwon\nhim\nhere\nit's\nyou've\nat\nshan\nis\nonly\nthey\nll\nours\nbut\nafter\nhasn\nfor\nwhen\ndown\nsame\nor\nyours\nt\nre\nitself","25":"import nltk, re\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\ndef expand_contractions(text):\n    text = re.sub(r\"can't\", \"can not\", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"'s\", \" \", text)\n    text = re.sub(r\"'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"'re\", \" are \", text)\n    text = re.sub(r\"'d\", \" would \", text)\n    text = re.sub(r\"'ll\", \" will \", text)\n    return text\n\ndef remove_url(text):\n    URL_REGEX = re.compile(r'''((http[s]?:\/\/)[^ <>'\"{}|\\^`[\\]]*)''')\n    return URL_REGEX.sub(r' ', text)\n\ndef remove_handles(text):\n    HANDLES_REGEX = re.compile(r'@\\S+')\n    return HANDLES_REGEX.sub(r' ', text)\n\ndef remove_incomplete_last_word(text):\n    INCOMPLETE_LAST_WORD_REGEX = re.compile(r'\\S+\u2026')\n    return INCOMPLETE_LAST_WORD_REGEX.sub(r' ', text )\n\nremove_punc = lambda x : re.sub(r\"\\W\", ' ', x)\n\nremove_num = lambda x : re.sub(r\"\\d\", ' ', x)\n\nremove_extra_spaces = lambda x : re.sub(r\"\\s+\", ' ', x)\n\nremove_shortwords = lambda x: ' '.join(word for word in x.split() if len(word) > 2)\n\nlower_case = lambda x : x.lower()\n\nwith open('stopwords.txt') as f:\n    sw = map(lambda x : x.strip(), f.readlines())\nstop_words = set(nltk.corpus.stopwords.words('english'))|set(sw)\nremove_stopwords = lambda x: ' '.join(word for word in x.split() if word not in stop_words)\n\nps = PorterStemmer()\nps_stem = lambda x: ' '.join(ps.stem(word) for word in x.split())\n\nwnl = WordNetLemmatizer()\nwnl_lemmatize = lambda x: ' '.join(wnl.lemmatize(word) for word in x.split())\n\ndef tag_pos(x):\n    tag_list =  nltk.pos_tag(nltk.word_tokenize(x))\n    pos = \"\"\n    for t in tag_list:\n        pos += t[0] +'(' + t[1] +')' + ' '\n    return pos\n\ndef cleanText(x, rsw, stm, lem, tgps):\n    x = str(x)\n    x = remove_url(x)\n    x = lower_case(x)\n    x = expand_contractions(x)\n    x = remove_punc(x)\n    x = remove_num(x)\n    x = remove_extra_spaces(x)\n    x = remove_shortwords(x)\n    \n    if rsw:\n        x = remove_stopwords(x)\n    if stm:\n        x = ps_stem(x)\n    if lem:\n        x = wnl_lemmatize(x)\n    if tgps:\n        x = tag_pos(x)\n    return x","26":"import time, datetime\r\n\r\ndef timestamp():\r\n    ts = time.time()\r\n    st = datetime.datetime.fromtimestamp(ts).strftime('%Y%m%d%H%M%S')\r\n    return st","27":"from IPython.display import clear_output\nfor i in range(n):\n    clear_output(wait=True)\n\n    print('Current Progress', i, '\/', n)","28":"function trim(x) {\n    return x.replace(\/^\\s+|\\s+$\/g, '');\n}","29":"a, b, *c, d = (1,2,3,4,5)\n\nprint(a) # 1\nprint(b) # 2\nprint(c) # [3,4]\nprint(d) # 5","30":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer \n\ndef sentiment_scores(sentence): \n    sia_obj = SentimentIntensityAnalyzer() \n    sentiment_dict = sia_obj.polarity_scores(sentence) \n    if sentiment_dict['compound'] >= 0.05 : \n        return 1\n    elif sentiment_dict['compound'] <= - 0.05 : \n        return -1\n    else : \n        return 0","31":"res = requests.get(url)\r\nif res.status_code == requests.codes.ok:\r\n    ressoup = bs4.BeautifulSoup(res.text, 'lxml')\r\n    elems = ressoup.select('.link')\r\n    elems[i].getText()\r\n    elems[i].get('attr')\r\nelse:\r\n    print('Something went wrong')","32":"import numpy as np\nimport pandas as pd\n\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\ndef buildWordCloud(text):\n    wordcloud = WordCloud(width = W, height = H,\n                background_color =BGC,\n                min_font_size = 10,\n                max_words = mw).generate(text)\n    plt.figure(figsize = (W\/\/100, H\/\/100), facecolor = None)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad = 0)\n    plt.show()","33":"# == tests equality\n# is test identity\n\nl1 = [1,2,3,4]\nl2 = [1,2,3,4]\n\nprint(l1 == l2) # True\nprint(l1 is l2) # False\n\nl3 = l1\nl1[0] = 5\n\nprint(l1 == l3) # True\nprint(l1 is l3) # True\nprint(l1, id(l1))       # [5, 2, 3, 4] 2391232377608\nprint(l3, id(l3))       # [5, 2, 3, 4] 2391232377608"}}