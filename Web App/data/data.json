{"Title":{"0":"ANN","1":"CNN (text)","2":"CustomScrollBar","3":"Deal with Large Numbers","4":"Dynamic Attributes in a Class","5":"Flatten a list","6":"Get Password in command line","7":"IgnoreWarnings","8":"RNN (text)","9":"Shebang","10":"Stopwords","11":"TextPreprocessing","12":"TimeStamp","13":"Unpacking","14":"Webscraping"},"Description":{"0":"(python)","1":"(python)","2":"(web)","3":"(python)","4":"(python)","5":"(python)","6":"(python)","7":"(python)","8":"(python)","9":"Shebang line for anaconda virtual environments (python)","10":"List of Stopwords(text)","11":"(python)","12":"(python)","13":"(python)","14":"Using bs4 (python)"},"Code":{"0":"from keras.models import Sequential\nfrom keras.laeyrs import Dense, Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras import optimizers\n# optimizers.SGD()\n# optimizers.RMSProp()\n# optimizers.Adagrad()\n# optimizers.Adadelta()\n# optimizers.Adam()\n\nes = EarlyStopping(monitor='loss', mode='min', verbose=1)\nfilepath = \"model.h5\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n\ndef build_network():\n    model = Sequential()\n    model.add(Dense(160,input_dim=40, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(320, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(365, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(125, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(25, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=, metrics=['accuracy'])\n    model.summary()\n    return model\n\nmodel = build_network()\n\nmodel.fit(scaled_data, train_label[0], validation_split=0.3, epochs=25, callbacks=[es, ckpt])\n\n\nfrom keras.models import load_model\nnew_model = load_model(\"model.h5\")","1":"from keras.models import Sequential\nfrom keras.laeyrs import Dense, Dropout, Flatten, Embedding, Convolution1D, MaxPooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import optimizers\n# optimizers.SGD()\n# optimizers.RMSProp()\n# optimizers.Adagrad()\n# optimizers.Adadelta()\n# optimizers.Adam()\n\nvocabulary_size = 7500\npad_length = 1000\ntokenizer = Tokenizer(num_words= vocabulary_size)\ntokenizer.fit_on_texts(X)\nsequences = tokenizer.texts_to_sequences(X)\npadded_sequences = pad_sequences(sequences, maxlen=pad_length)\n\nes = EarlyStopping(monitor='loss', mode='min', verbose=1)\nfilepath = \"model.h5\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n\ndef build_network():\n    model = Sequential()\n    model.add(Embedding(vocabulary_size, 1024, input_length=pad_length))\n    model.add(Convolution1D(1024, kernel_size=5, activation='tanh', strides=2))\n    model.add(MaxPooling1D(pool_size=5))\n    model.add(Dense(160,input_dim=40, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(320, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(365, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(125, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(25, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=, metrics=['accuracy'])\n    model.summary()\n    return model\n\nmodel = build_network()\n\nmodel.fit(scaled_data, train_label[0], validation_split=0.3, epochs=25, callbacks=[es, ckpt])\n\n\nfrom keras.models import load_model\nnew_model = load_model(\"model.h5\")","2":"\/* width *\/\n::-webkit-scrollbar {\n  width: 10px;\n}\n\n\/* Track *\/\n::-webkit-scrollbar-track {\n  background: #f1f1f1; \n}\n\n\/* Handle *\/\n::-webkit-scrollbar-thumb {\n  background: #888; \n}\n\n\/* Handle on hover *\/\n::-webkit-scrollbar-thumb:hover {\n  background: #555; \n}","3":"num1 = 100_000_000_000  # Visually chunk large numbers\nnum2 =   1_000_000_000  # without affecting the functionality\ntotal = num1 + num2\nprint(f'{total:,}')     # 101,000,000,000","4":"class Person():\n    pass\n\nperson = Person()\n\np_key = 'name'\np_value = 'Ritvik'\n\n\nsetattr(person, p_key, p_value)\nname = getattr(person, p_key)","5":"flat_list = [item for sublist in l for item in sublist]","6":"from getpass import getpass\nusername = input('Enter Username...')   # User Input is displayed in the commandline\npassword = getpass('Enter Password...') # User Input is not displayed in the commandline","7":"import warnings\r\nwarnings.filterwarnings(\"ignore\")","8":"from keras.models import Sequential\nfrom keras.laeyrs import Dense, Dropout, LSTM, Embedding\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import optimizers\n# optimizers.SGD()\n# optimizers.RMSProp()\n# optimizers.Adagrad()\n# optimizers.Adadelta()\n# optimizers.Adam()\n\nvocabulary_size = 7500\npad_length = 1000\ntokenizer = Tokenizer(num_words= vocabulary_size)\ntokenizer.fit_on_texts(X)\nsequences = tokenizer.texts_to_sequences(X)\npadded_sequences = pad_sequences(sequences, maxlen=pad_length)\n\nes = EarlyStopping(monitor='loss', mode='min', verbose=1)\nfilepath = \"model.h5\"\nckpt = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n\ndef build_network():\n    model = Sequential()\n    model.add(Embedding(vocabulary_size, 1024, input_length=pad_length))\n    model.add(LSTM(1024, dropout=0.2, recurrent_dropout=0.2))\n    model.add(Dense(160,input_dim=40, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(320, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(365, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(125, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(25, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer=, metrics=['accuracy'])\n    model.summary()\n    return model\n\nmodel = build_network()\n\nmodel.fit(scaled_data, train_label[0], validation_split=0.3, epochs=25, callbacks=[es, ckpt])\n\n\nfrom keras.models import load_model\nnew_model = load_model(\"model.h5\")","9":"#!D:\\Users\\Ritvik\\Anaconda3\\envs\\datascience\\python.exe","10":"a\nabout\nabove\nacross\nafter\nafterwards\nagain\nagainst\nall\nalmost\nalone\nalong\nalready\nalso\nalthough\nalways\nam\namong\namongst\namoungst\namount\nan\nand\nanother\nany\nanyhow\nanyone\nanything\nanyway\nanywhere\nare\naround\nas\nat\nback\nbe\nbecame\nbecause\nbecome\nbecomes\nbecoming\nbeen\nbefore\nbeforehand\nbehind\nbeing\nbelow\nbeside\nbesides\nbetween\nbeyond\nbill\nboth\nbottom\nbut\nby\ncall\ncan\ncannot\ncant\nco\ncomputer\ncon\ncould\ncouldnt\ncry\nde\ndescribe\ndetail\ndo\ndone\ndown\ndue\nduring\neach\neg\neight\neither\neleven\nelse\nelsewhere\nempty\nenough\netc\neven\never\nevery\neveryone\neverything\neverywhere\nexcept\nfew\nfifteen\nfify\nfill\nfind\nfire\nfirst\nfive\nfor\nformer\nformerly\nforty\nfound\nfour\nfrom\nfront\nfull\nfurther\nget\ngive\ngo\nhad\nhas\nhasnt\nhave\nhe\nhence\nher\nhere\nhereafter\nhereby\nherein\nhereupon\nhers\nherse\"\nhim\nhimse\"\nhis\nhow\nhowever\nhundred\ni\nie\nif\nin\ninc\nindeed\ninterest\ninto\nis\nit\nits\nitse\"\nkeep\nlast\nlatter\nlatterly\nleast\nless\nltd\nmade\nmany\nmay\nme\nmeanwhile\nmight\nmill\nmine\nmore\nmoreover\nmost\nmostly\nmove\nmuch\nmust\nmy\nmyse\"\nname\nnamely\nneither\nnever\nnevertheless\nnext\nnine\nno\nnobody\nnone\nnoone\nnor\nnot\nnothing\nnow\nnowhere\nof\noff\noften\non\nonce\none\nonly\nonto\nor\nother\nothers\notherwise\nour\nours\nourselves\nout\nover\nown\npart\nper\nperhaps\nplease\nput\nrather\nre\nsame\nsee\nseem\nseemed\nseeming\nseems\nserious\nseveral\nshe\nshould\nshow\nside\nsince\nsincere\nsix\nsixty\nso\nsome\nsomehow\nsomeone\nsomething\nsometime\nsometimes\nsomewhere\nstill\nsuch\nsystem\ntake\nten\nthan\nthat\nthe\ntheir\nthem\nthemselves\nthen\nthence\nthere\nthereafter\nthereby\ntherefore\ntherein\nthereupon\nthese\nthey\nthick\nthin\nthird\nthis\nthose\nthough\nthree\nthrough\nthroughout\nthru\nthus\nto\ntogether\ntoo\ntop\ntoward\ntowards\ntwelve\ntwenty\ntwo\nun\nunder\nuntil\nup\nupon\nus\nvery\nvia\nwas\nwe\nwell\nwere\nwhat\nwhatever\nwhen\nwhence\nwhenever\nwhere\nwhereafter\nwhereas\nwhereby\nwherein\nwhereupon\nwherever\nwhether\nwhich\nwhile\nwhither\nwho\nwhoever\nwhole\nwhom\nwhose\nwhy\nwill\nwith\nwithin\nwithout\nwould\nyet\nyou\nyour\nyours\nyourself\nyourselves\nwhom\nm\ncouldn't\nuntil\nduring\nunder\nwasn\nboth\nout\ndidn\nbefore\naren't\nweren\nwhy\neach\nmightn\ndoesn\nbelow\nshould\nhers\nherself\nwho\ntoo\nmost\nand\nso\nabout\nshould've\nme\nyou'll\nno\nup\nshouldn\ncan\noff\nthan\njust\nonce\nain\nisn\nwon't\nthat'll\nthose\ndon\nhadn't\nisn't\nwhich\nshouldn't\nto\nthemselves\nyourself\nwas\non\nother\no\nhad\nher\nourselves\nmightn't\nhe\ndoes\nany\nyour\nthese\ndoing\nan\nby\nthrough\nwe\nfrom\nmy\nits\nit\nhaven\nma\nhasn't\nbeing\nve\nsome\nshan't\nover\nmustn't\nhimself\ntheirs\nwhere\nif\nmyself\ndo\ndon't\nwouldn't\nthe\nof\nneedn\nwere\naren\nin\nwasn't\nfew\nnow\nmustn\ns\ny\nyou're\ntheir\nbeen\nour\ni\nbecause\nnot\nbe\nthen\nagainst\nd\nhaven't\nhis\nagain\nhow\nall\nas\nweren't\nmore\nwouldn\nwill\ninto\nthis\nyou\nshe's\nare\nyourselves\ndidn't\nabove\nown\nneedn't\nwith\nyou'd\nhave\nam\nfurther\nthere\nvery\nnor\ndoesn't\nwhat\ndid\nthat\nwhile\nthem\nsuch\nhadn\nshe\nbetween\nhaving\ncouldn\nhas\na\nwon\nhim\nhere\nit's\nyou've\nat\nshan\nis\nonly\nthey\nll\nours\nbut\nafter\nhasn\nfor\nwhen\ndown\nsame\nor\nyours\nt\nre\nitself","11":"import nltk, re\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\ndef spell_correct(text):\n    text = re.sub(r\"can't\", \"can not\", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"'s\", \" \", text)\n    text = re.sub(r\"'ve\", \" have \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"'re\", \" are \", text)\n    text = re.sub(r\"'d\", \" would \", text)\n    text = re.sub(r\"'ll\", \" will \", text)\n    text = re.sub(r\"\u0000s\", \"0\", text)    \n    return text\n\ndef remove_url(text):\n    URL_REGEX = re.compile(r'''((http[s]?:\/\/)[^ <>'\"{}|\\^`[\\]]*)''')\n    return URL_REGEX.sub(r' ', text)\n\ndef remove_handles(text):\n    HANDLES_REGEX = re.compile(r'@\\S+')\n    return HANDLES_REGEX.sub(r' ', text)\n\ndef remove_incomplete_last_word(text):\n    INCOMPLETE_LAST_WORD_REGEX = re.compile(r'\\S+\u2026')\n    return INCOMPLETE_LAST_WORD_REGEX.sub(r' ', text )\n\nremove_punc = lambda x : re.sub(r\"\\W\", ' ', x)\n\nremove_num = lambda x : re.sub(r\"\\d\", ' ', x)\n\nremove_extra_spaces = lambda x : re.sub(r\"\\s+\", ' ', x)\n\nremove_shortwords = lambda x: ' '.join(word for word in x.split() if len(word) > 2)\n\nlower_case = lambda x : x.lower()\n\nwith open('stopwords.txt') as f:\n    sw = map(lambda x : x.strip(), f.readlines())\nstop_words = set(nltk.corpus.stopwords.words('english'))|set(sw)\nremove_stopwords = lambda x: ' '.join(word for word in x.split() if word not in stop_words)\n\nps = PorterStemmer()\nps_stem = lambda x: ' '.join(ps.stem(word) for word in x.split())\n\nwnl = WordNetLemmatizer()\nwnl_lemmatize = lambda x: ' '.join(wnl.lemmatize(word) for word in x.split())\n\ndef tag_pos(x):\n    tag_list =  nltk.pos_tag(nltk.word_tokenize(x))\n    pos = \"\"\n    for t in tag_list:\n        pos += t[0] +'(' + t[1] +')' + ' '\n    return pos\n\ndef cleanText(x, rsw, stm, lem, tgps):\n    x = str(x)\n    x = remove_url(x)\n    x = lower_case(x)\n    x = spell_correct(x)\n    x = remove_punc(x)\n    x = remove_num(x)\n    x = remove_extra_spaces(x)\n    x = remove_shortwords(x)\n    \n    if rsw:\n        x = remove_stopwords(x)\n    if stm:\n        x = ps_stem(x)\n    if lem:\n        x = wnl_lemmatize(x)\n    if tgps:\n        x = tag_pos(x)\n    return x","12":"import time, datetime\r\n\r\ndef timestamp():\r\n    ts = time.time()\r\n    st = datetime.datetime.fromtimestamp(ts).strftime('%Y%m%d%H%M%S')\r\n    return st","13":"a, b, *c, d = (1,2,3,4,5)\n\nprint(a) # 1\nprint(b) # 2\nprint(c) # [3,4]\nprint(d) # 5","14":"res = requests.get(url)\r\nif res.status_code == requests.codes.ok:\r\n    ressoup = bs4.BeautifulSoup(res.text, 'lxml')\r\n    elems = ressoup.select('.link')\r\n    elems[i].getText()\r\n    elems[i].get('attr')\r\nelse:\r\n    print('Something went wrong')"}}